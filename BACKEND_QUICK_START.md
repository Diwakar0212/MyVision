# ğŸ¯ MyVision Backend - Quick Reference

## âœ… Installation Complete!

All dependencies have been installed successfully. Your FastAPI backend is ready to use!

## ğŸ“¦ What You Have

### Backend Structure:
```
backend/
â”œâ”€â”€ main.py                  # FastAPI server with 3 YOLO models
â”œâ”€â”€ requirements.txt         # All dependencies (âœ… installed)
â”œâ”€â”€ models/                  # âš ï¸ ADD YOUR MODELS HERE!
â”‚   â”œâ”€â”€ yolov8m.pt          (YOLOv8m base model)
â”‚   â”œâ”€â”€ traffic_lights.pt    (your model from Colab)
â”‚   â””â”€â”€ zebra_crossing.pt    (your model from Colab)
â””â”€â”€ README.md               # Detailed documentation
```

## ğŸš¨ IMPORTANT: Add Your Models!

Before using the API, copy your trained models from Google Colab:

1. Download from Colab:
   - `yolov8m.pt` â†’ place in `backend/models/`
   - `traffic lights.pt` â†’ rename to `traffic_lights.pt`
   - `zebra crossing.pt` â†’ rename to `zebra_crossing.pt`

2. Copy all three models to: `backend/models/`

## ğŸš€ Start the Server

### Option 1: Quick Start Script
```bash
python start_backend.py
```

### Option 2: Direct Command
```bash
cd backend
python main.py
```

### Option 3: Using Uvicorn
```bash
uvicorn backend.main:app --reload --host 0.0.0.0 --port 8000
```

## ğŸŒ Access the API

Once running:
- **API Base**: http://localhost:8000
- **Interactive Docs**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000/health

## ğŸ“¡ API Endpoints

### 1. Health Check
```bash
GET http://localhost:8000/health
```

### 2. Image Detection
```bash
POST http://localhost:8000/api/detect/image
Content-Type: multipart/form-data

# Form data:
- file: <your-image.jpg>
- confidence: 0.4 (optional)
```

**Response:**
```json
{
  "success": true,
  "detections": {
    "objects": [{"label": "car", "confidence": 0.85, ...}],
    "traffic_lights": [{"label": "red", "confidence": 0.92, ...}],
    "zebra_crossings": [{"label": "zebra_crossing", ...}]
  },
  "voice_description": "I see 2 cars, 1 person. Red traffic light ahead, please stop.",
  "annotated_image": "data:image/jpeg;base64,..."
}
```

### 3. Video Analysis
```bash
POST http://localhost:8000/api/detect/video
Content-Type: multipart/form-data

# Form data:
- file: <your-video.mp4>
- confidence: 0.4
- sample_rate: 10
```

### 4. Live Detection (WebSocket)
```javascript
const ws = new WebSocket('ws://localhost:8000/api/detect/live');

// Send frame as base64
ws.send(frameBase64);

// Receive detections
ws.onmessage = (event) => {
  const data = JSON.parse(event.data);
  console.log(data.detections);
  console.log(data.voice_description);
};
```

## ğŸ§ª Test the API

### Using cURL:
```bash
# Health check
curl http://localhost:8000/health

# Image detection
curl -X POST "http://localhost:8000/api/detect/image" \
  -F "file=@test_image.jpg" \
  -F "confidence=0.4"
```

### Using Browser:
Visit: http://localhost:8000/docs for interactive testing

### Using Python:
```python
import requests

# Upload image
files = {'file': open('test.jpg', 'rb')}
response = requests.post(
    'http://localhost:8000/api/detect/image',
    files=files
)
print(response.json())
```

## ğŸ”§ How It Works

Your backend uses **3 YOLO models** in sequence:

1. **YOLOv8m** (auto-downloaded)
   - Detects: cars, people, bikes, etc.
   - Excludes: traffic lights (class 9)

2. **Traffic Light Model** (your custom model)
   - Detects: red, green, yellow lights
   - Classes: 2, 3, 4

3. **Zebra Crossing Model** (your custom model)
   - Detects: zebra crossings
   - Class: 8

All detections are combined and returned with:
- Bounding boxes
- Confidence scores
- Voice description for accessibility

## ğŸ“Š Model Detection Flow

```
Image Input
    â†“
[YOLOv8m] â†’ Objects (cars, people)
    â†“
[Traffic Lights] â†’ Red/Green/Yellow
    â†“
[Zebra Crossing] â†’ Crosswalks
    â†“
Merge Results + Generate Voice Description
    â†“
Return Annotated Image + JSON
```

## ğŸ¤ Voice Descriptions

The API automatically generates natural language descriptions:

```python
# Example outputs:
"I see 2 cars, 1 person. Red traffic light ahead, please stop."
"Green traffic light, you can go. Zebra crossing detected ahead."
"I see 1 car, 3 people."
```

Perfect for integration with your voice-controlled React app!

## ğŸ”— Connect to React Frontend

In your React app, make API calls:

```typescript
// Upload image for detection
const detectImage = async (file: File) => {
  const formData = new FormData();
  formData.append('file', file);
  
  const response = await fetch('http://localhost:8000/api/detect/image', {
    method: 'POST',
    body: formData
  });
  
  const data = await response.json();
  
  // Show annotated image
  setImageSrc(data.annotated_image);
  
  // Speak voice description
  speakText(data.voice_description);
  
  // Display detections
  setDetections(data.detections);
};
```

## ğŸ› Troubleshooting

### Models not loading?
- Check if files exist in `backend/models/`
- Verify filenames: `traffic_lights.pt` and `zebra_crossing.pt`
- Check file paths in `main.py`

### Port 8000 already in use?
Change port in `main.py`:
```python
uvicorn.run(app, port=8001)  # Use 8001 instead
```

### CORS errors from React?
Already configured! The backend allows:
- http://localhost:5173
- http://localhost:8080
- http://localhost:8085

### GPU not detected?
No problem! PyTorch will automatically use CPU. Models still work, just slower.

## ğŸ“ Next Steps

1. âœ… Copy your trained models to `backend/models/`
2. âœ… Start the backend server
3. âœ… Test with http://localhost:8000/docs
4. âœ… Connect your React frontend
5. âœ… Build amazing vision assistance features!

## ğŸ’¡ Tips

- Use `confidence=0.3` for more detections
- Use `confidence=0.6` for higher accuracy
- Video analysis samples every Nth frame (adjust `sample_rate`)
- WebSocket is best for real-time camera feeds
- REST API is best for uploaded images/videos

---

**Need help?** Check the detailed README in `backend/README.md`

**Server Status:** ğŸŸ¢ Running at http://localhost:8000
